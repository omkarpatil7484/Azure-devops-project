ğŸ§  High Memory Usage Runbook

This runbook explains what actions to take when a High Memory Alert is triggered in a Kubernetes cluster.

1ï¸âƒ£ Check Node-Level Memory Usage

Run the following command to check how much memory each node is using:

kubectl top nodes


ğŸ“Œ If memory usage is above 80%, continue to the next step.

2ï¸âƒ£ Identify Which Pod Is Using the Most Memory
kubectl top pod -A


ğŸ“Œ Example output:

NAMESPACE     NAME                                     CPU   MEMORY
default       devops-demo-app                          1m    42Mi
monitoring    grafana                                  6m    672Mi
monitoring    prometheus                               23m   244Mi


â¡ï¸ Identify the pod consuming the highest memory.

3ï¸âƒ£ Inspect the Pod
kubectl describe pod <pod-name> -n <namespace>


Check for:

ğŸš¨ High restart count

âŒ OOMKilled (Out Of Memory) errors

âš ï¸ Any warnings or error logs

4ï¸âƒ£ Fix Actions
ğŸ”§ Option 1 â€” Restart the Pod
kubectl delete pod <pod-name> -n <namespace>


â¡ï¸ Kubernetes will automatically create a replacement pod.

ğŸ”§ Option 2 â€” Increase Memory Resource Limits

Modify Deployment YAML or Helm chart:

resources:
  limits:
    memory: "1Gi"
  requests:
    memory: "512Mi"


â¡ï¸ Apply the updated configuration:

kubectl apply -f deployment.yaml

ğŸ”§ Option 3 â€” Scale the Deployment

If the workload is high, increase replicas:

kubectl scale deploy <deploy-name> --replicas=2 -n <namespace>

5ï¸âƒ£ If the issue continues:

Notify the team (DevOps Lead / SRE)

Scale the cluster (add new node pool or increase node size)

ğŸ“£ Slack / Teams Update Template
ğŸ”” Alert Triggered: High Memory Usage  
ğŸ“Œ Pod: <pod-name>  
ğŸ•’ Time: <timestamp>  
ğŸ” Action Taken: Restarted / Scaled / Updated limits  
ğŸ“ˆ Next Review: After 10 minutes

âœ… Resolution Checklist

âœ” Memory usage returned to normal (<80%)
âœ” Pod stable (no restarts/errors)
âœ” No new alerts for 15â€“20 minutes
